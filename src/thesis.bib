Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Verma,
author = {{Verma, TS; Pearl}, Judea},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/verma\_Peral\_IC\_R150.pdf:pdf},
journal = {Uncertainty in Artificial Intelligence},
keywords = {Bayesian Networks,causal networks},
mendeley-tags = {Bayesian Networks,causal networks},
pages = {255--268},
title = {{Technical report r-150}},
volume = {6},
year = {1991}
}
@article{Abellan2006,
author = {Abell\'{a}n, Joaqu\'{\i}n and G\'{o}mez-Olmedo, Manuel and Moral, Seraf\'{\i}n},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/variations\_on\_PC41\_paper.pdf:pdf},
isbn = {8086742148},
journal = {Third European Workshop on Probabilistic Graphical Models, 12-15 September 2006, Prague, Czech Republic. Electronic Proceedings},
pages = {1--8},
title = {{Some Variations on the PC Algorithm}},
year = {2006}
}
@article{Kanazawa1995,
author = {Kanazawa, Keiji and Koller, Daphne and Russell, Stuart},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/stochastic\_simulation\_dbn\_koller.pdf:pdf},
journal = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
pages = {346--351},
title = {{Stochastic simulation algorithms for dynamic probabilistic networks}},
year = {1995}
}
@book{Williamson2005,
address = {Oxford},
author = {Williamson, Jon},
publisher = {Oxford University Press},
title = {{Bayesian Nets and Causality}},
year = {2005}
}
@article{Friedman1998,
abstract = {Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains.},
author = {Friedman, Nir and Murphy, Kevin and Russell, Stuart},
doi = {10.1111/j.1469-7580.2008.00962.x},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/freidman\_dynamic\_bayes.pdf:pdf},
isbn = {155860555X},
journal = {Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence},
pages = {139--147},
title = {{Learning the structure of dynamic probabilistic networks}},
url = {http://dl.acm.org/citation.cfm?id=2074094.2074111},
year = {1998}
}
@article{Robinson2010,
abstract = {Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN struc- ture learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a non- stationary dynamic Bayesian network, in which the conditional dependence structure of the under- lying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organismâ€™s development, neural pathways during learning, and traffic patterns during the day. We define the non-stationary DBN model, present anMCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data.},
author = {Robinson, Jw and Hartemink, Aj},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/learning\_non\_stationary\_dbn\_robinson10a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning \ldots},
keywords = {bayesian networks,graphical models,model selection,monte,structure learning},
pages = {3647--3680},
title = {{Learning non-stationary dynamic Bayesian networks}},
url = {http://dl.acm.org/citation.cfm?id=1953047},
volume = {11},
year = {2010}
}
@book{Freedman2010,
address = {New York},
author = {Freedman, David A.},
keywords = {Statistics},
mendeley-tags = {Statistics},
publisher = {Cambridge University Press},
title = {{Statistical Models and Causal Inference}},
year = {2010}
}
@article{Robinson2008,
author = {Robinson, Jw and Hartemink, Aj},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/non-stationary-dynamic-bayesian-networks.pdf:pdf},
journal = {Nips},
pages = {1--8},
title = {{Non-stationary dynamic Bayesian networks.}},
url = {https://papers.nips.cc/paper/3571-non-stationary-dynamic-bayesian-networks.pdf},
year = {2008}
}
@article{Rish2000,
author = {Rish, Irina and Singh, M},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/slides\_survey\_vittorio\_lecture12.pdf:pdf},
journal = {IBM Watson Research Center},
title = {{A tutorial on inference and learning in Bayesian networks}},
url = {http://www.ee.columbia.edu/~vittorio/Lecture12.pdf},
year = {2000}
}
@article{Giovani2014,
author = {{Heckerman, David; Geiger, Dan; Chickering}, David M.},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/chickering\_Learning\_bayes\_networks-008.pdf:pdf},
journal = {Workshop on Knowledge Discovery in Databases},
keywords = {active suspension,genetic algorithm,h,multiobjective control},
pages = {85--96},
title = {{Learning Bayesian networks: The combination of knowledge and statistical data}},
year = {1994}
}
@article{Studeny2011,
abstract = {We review three vector encodings of Bayesian network structures. The first one has recently been applied by Jaakkola 2010, the other two use special integral vectors formerly introduced, called imsets [Studeny 2005, Studeny 2010]. The central topic is the comparison of outer polyhedral approximations of the corresponding polytopes. We show how to transform the inequalities suggested by Jaakkola et al. to the framework of imsets. The result of our comparison is the observation that the implicit polyhedral approximation of the standard imset polytope suggested in [Studeny 2011] gives a closer approximation than the (transformed) explicit polyhedral approximation from [Jaakkola 2010]. Finally, we confirm a conjecture from [Studeny 2011] that the above-mentioned implicit polyhedral approximation of the standard imset polytope is an LP relaxation of the polytope.},
archivePrefix = {arXiv},
arxivId = {1107.4708},
author = {Studeny, Milan and Haws, David},
eprint = {1107.4708},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/polyhedral\_approx\_of\_polytopes\_studeny.pdf:pdf},
pages = {1--31},
title = {{On polyhedral approximations of polytopes for learning Bayes nets}},
url = {http://arxiv.org/abs/1107.4708},
year = {2011}
}
@article{Ghahramani2004,
abstract = {Abstract We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised  ... $\backslash$n},
author = {Ghahramani, Zoubin},
doi = {10.1007/978-3-540-28650-9\_5},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/unsupervisedlearning\_survey.pdf:pdf},
isbn = {978-3-540-23122-6},
issn = {1745-1337},
journal = {Advanced Lectures on Machine Learning},
number = {Chapter 5},
pages = {72--112},
pmid = {24729780},
title = {{Unsupervised Learning BT  - Advanced Lectures on Machine Learning}},
url = {http://link.springer.com/10.1007/978-3-540-28650-9\_5$\backslash$npapers3://publication/doi/10.1007/978-3-540-28650-9\_5},
volume = {3176},
year = {2004}
}
@article{Acid2004,
abstract = {Due to the uncertainty of many of the factors that influence the performance of an emergency medical service, we propose using Bayesian networks to model this kind of system. We use different algorithms for learning Bayesian networks in order to build several models, from the hospital manager's point of view, and apply them to the specific case of the emergency service of a Spanish hospital. This first study of a real problem includes preliminary data processing, the experiments carried out, the comparison of the algorithms from different perspectives, and some potential uses of Bayesian networks for management problems in the health service.},
author = {Acid, S and de Campos, L M and Fern\'{a}ndez-Luna, J M and Rodr\'{\i}guez, S and {Mar\'{\i}a Rodr\'{\i}guez}, J and {Luis Salcedo}, J},
doi = {10.1016/j.artmed.2003.11.002},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/algorithim\_case\_study\_aime04.pdf:pdf},
isbn = {09333657},
issn = {0933-3657},
journal = {Artificial Intelligence in Medicine},
keywords = {Bayesian belief networks,Bayesian inference,Bayesian network,Bayesian networks,Bayesian probabilities,Emergency medical service,Independence,Learning algorithm,Management decision support in the health service,Scoring functions},
number = {3},
pages = {215--232},
pmid = {15081073},
title = {{A comparison of learning algorithms for Bayesian networks: a case study based on data from an emergency medical service}},
url = {https://ejournal.csiro.au/cgi-bin/sciserv.pl?collection=journals\&journal=09333657\&issue=v30i0003\&article=215\_acolafdfaems},
volume = {30},
year = {2004}
}
@article{Chickering2004,
author = {Chickering, DM and Heckerman, D and Meek, C},
file = {:home/nathanael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chickering, Heckerman, Meek - 2004 - Large-sample learning of Bayesian networks is NP-hard.pdf:pdf},
journal = {The Journal of Machine  \ldots},
keywords = {bayesian networks,statistics},
mendeley-tags = {bayesian networks,statistics},
title = {{Large-sample learning of Bayesian networks is NP-hard}},
url = {http://dl.acm.org/citation.cfm?id=1044703},
year = {2004}
}
@article{Silander2006,
abstract = {We study the problem of learning the best Bayesian network structure with respect to a decomposable score such as BDe, BIC or AIC. This problem is known to be hboxNP-hard, which means that solving it becomes quickly infeasible as the number of variables increases. Nevertheless, in this paper we show that it is possible to learn the best Bayesian network structure with over 30 variables, which covers many practically interesting cases. Our algorithm is less complicated and more efficient than the techniques presented earlier. It can be easily parallelized, and offers a possibility for efficient exploration of the best networks consistent with different variable orderings. In the experimental part of the paper we compare the performance of the algorithm to the previous state-of-the-art algorithm. Free source-code and an online-demo can be found at http://b-course.hiit.fi/bene.},
archivePrefix = {arXiv},
arxivId = {1206.6875},
author = {Silander, Tomi and Myllym\"{a}ki, Petri},
eprint = {1206.6875},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/silander\_gobally\_optimal\_bn.pdf:pdf},
journal = {Networks},
keywords = {learning,statistics \& optimisation},
pages = {445--452},
title = {{A simple approach for finding the globally optimal Bayesian network structure}},
url = {http://eprints.pascal-network.org/archive/00002135/},
year = {2006}
}
@article{Kwisthout,
author = {Kwisthout, Johan},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/campos\_computational\_complexity.pdf:pdf},
title = {{Lecture notes : Computational Complexity of Bayesian Networks}}
}
@article{Zhou2007,
abstract = {Probabilistic graphical models combine the graph theory and probability theory to give a multivariate statistical modeling. They provide a unified description of uncertainty using probability and complexity using the graphical model. Especially, graphical models provide the following several useful properties: - Graphical models provide a simple and intuitive interpretation of the structures of probabilistic models. On the other hand, they can be used to design and motivate new models. - Graphical models provide additional insights into the properties of the model, including the conditional independence properties. - Complex computations which are required to perform inference and learning in sophisticated models can be expressed in terms of graphical manipulations, in which the underlying mathematical expressions are carried along implicitly. The graphical models have been applied to a large number of fields, including bioinformatics, social science, control theory, image processing, marketing analysis, among others. However, structure learning for graphical models remains an open challenge, since one must cope with a combinatorial search over the space of all possible structures. In this paper, we present a comprehensive survey of the existing structure learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1111.6925},
author = {Zhou, Yang},
eprint = {1111.6925},
file = {:home/nathanael/sfsu/fall\_2015/mach\_learn\_872/graph\_models/papers/zhang\_structure\_survey\_v1.pdf:pdf},
journal = {Structure},
title = {{Structure Learning of Probabilistic Graphical Models : A Comprehensive Survey}},
url = {http://arxiv.org/abs/1111.6925},
year = {2007}
}
